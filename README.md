# Gradient-Descent-for-Linear-Regression
An educational Jupyter Notebook that implements Batch Gradient Descent to train a simple Linear Regression model from scratch â€” visualizing how the cost function decreases and how the line fits the data step by step. Perfect for understanding how machine learning models actually learn!
ðŸ“ˆ Gradient Descent for Linear Regression
This repository contains an educational Jupyter Notebook that demonstrates how to implement Batch Gradient Descent to train a simple Linear Regression model from scratch. It is designed for beginners who want to deeply understand how gradient descent works step by step, how the cost function behaves, and how model parameters (w and b) converge to their optimal values.

ðŸš€ Features
âœ… Implementation of Linear Regression from scratch
âœ… Uses Mean Squared Error (MSE) cost function
âœ… Derivation of partial derivatives with respect to w and b
âœ… Batch Gradient Descent algorithm step-by-step
âœ… Visualizations of the cost surface, contour plot, and line fitting
âœ… Example dataset to see training in action
âœ… Well-commented, easy-to-follow Python code in Jupyter Notebook

ðŸ§® What Youâ€™ll Learn
Linear Regression Model: How to define a hypothesis function h(x) = wx + b.

Cost Function: Why we use Mean Squared Error (MSE) and the 1/2m factor.

Partial Derivatives: How to derive the gradient formulas step by step.

Gradient Descent Algorithm: How to iteratively update w and b to minimize the cost.

Visualization: How cost decreases and the fitted line improves with each update.

The notebook includes:

Line plot: Shows how the fitted line improves after each iteration.

Contour plot: Visualizes how w and b move towards the global minimum.

Surface plot: Shows the cost functionâ€™s bowl-shaped surface.

Clone the repository
https://github.com/junaidshah2001/Gradient-Descent-for-Linear-Regression
